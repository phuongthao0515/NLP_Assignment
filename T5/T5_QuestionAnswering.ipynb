{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59caf8ad",
   "metadata": {},
   "source": [
    "### T5 MODEL - TASK: QUESTION ANSWERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef1f48",
   "metadata": {},
   "source": [
    "#### SETUP\n",
    "\n",
    "##### DATASET: SQuAD v1.1\n",
    "- Benchmark dataset for reading comprehension.\n",
    "- Contains 100,000+ crowd-sourced QA pairs from Wikipedia.\n",
    "- Each answer is a span of text from a related paragraph.\n",
    "- Commonly used to evaluate models' ability to extract information.\n",
    "\n",
    "##### MODEL: T5-Small\n",
    "- A lightweight version of the T5 model (~60M parameters).\n",
    "- Treats all NLP tasks as text-to-text problems.\n",
    "- Suitable for fine-tuning with limited computational resources.\n",
    "- Used here for extractive QA on the SQuAD v1.1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9b84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43aab04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.system(\"your_command_here 2>/dev/null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33191e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47253d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    TaskType,\n",
    "    PromptTuningInit\n",
    ")\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d0310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e64b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c202753b",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84866ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs, targets = [], []\n",
    "\n",
    "    for q, c, a in zip(examples[\"question\"], examples[\"context\"], examples[\"answers\"]):\n",
    "        if a[\"text\"]:\n",
    "            inputs.append(f\"question: {q} context: {c}\")\n",
    "            targets.append(a[\"text\"][0])\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=64, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels_input_ids = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_input_ids\n",
    "    model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n",
    "\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea842781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d780e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply preprocessing\n",
    "dataset = dataset.filter(lambda x: len(x[\"answers\"][\"text\"]) > 0)\n",
    "dataset = dataset.map(preprocess_function, batched=True, batch_size=100, remove_columns=dataset[\"train\"].column_names)\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"decoder_attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d0f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metric (SQuAD)\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # If predictions is a tuple (sometimes logits + other outputs), take the first element\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    # Convert predictions to tensor if not already\n",
    "    pred_tensor = torch.tensor(predictions) if not isinstance(predictions, torch.Tensor) else predictions\n",
    "    label_tensor = torch.tensor(labels) if not isinstance(labels, torch.Tensor) else labels\n",
    "\n",
    "    # If predictions are logits (3D: batch_size x seq_len x vocab_size), take argmax over vocab dim\n",
    "    if pred_tensor.ndim == 3:\n",
    "        pred_ids = torch.argmax(pred_tensor, dim=-1)\n",
    "    else:\n",
    "        pred_ids = pred_tensor\n",
    "\n",
    "    # If labels have -100 as padding, replace with tokenizer.pad_token_id for decoding\n",
    "    labels_ids = label_tensor.clone()\n",
    "    pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "    labels_ids[labels_ids == -100] = pad_token_id\n",
    "\n",
    "    pred_texts = []\n",
    "    label_texts = []\n",
    "\n",
    "    for pred_seq, label_seq in zip(pred_ids, labels_ids):\n",
    "        # Clip indices that are out of tokenizer vocab range\n",
    "        vocab_size = len(tokenizer)\n",
    "        pred_seq_clipped = [token if 0 <= token < vocab_size else pad_token_id for token in pred_seq.tolist()]\n",
    "        label_seq_clipped = [token if 0 <= token < vocab_size else pad_token_id for token in label_seq.tolist()]\n",
    "\n",
    "        # Decode sequences safely\n",
    "        pred_text = tokenizer.decode(pred_seq_clipped, skip_special_tokens=True)\n",
    "        label_text = tokenizer.decode(label_seq_clipped, skip_special_tokens=True)\n",
    "\n",
    "        pred_texts.append(pred_text)\n",
    "        label_texts.append(label_text)\n",
    "\n",
    "    predictions_list = [{\"prediction_text\": p, \"id\": str(i)} for i, p in enumerate(pred_texts)]\n",
    "    references_list = [{\"answers\": {\"text\": [l], \"answer_start\": [0]}, \"id\": str(i)} for i, l in enumerate(label_texts)]\n",
    "\n",
    "\n",
    "    return metric.compute(predictions=predictions_list, references=references_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6bde5",
   "metadata": {},
   "source": [
    "#### FREEZE TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f8e326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training with FREEZE TUNING ====\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27380' max='27380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27380/27380 1:59:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.389300</td>\n",
       "      <td>0.447231</td>\n",
       "      <td>61.201514</td>\n",
       "      <td>75.796557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.452800</td>\n",
       "      <td>0.445630</td>\n",
       "      <td>61.229896</td>\n",
       "      <td>75.789781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.418400</td>\n",
       "      <td>0.444857</td>\n",
       "      <td>61.210974</td>\n",
       "      <td>75.748865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.444233</td>\n",
       "      <td>61.258278</td>\n",
       "      <td>75.793609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.365400</td>\n",
       "      <td>0.443798</td>\n",
       "      <td>61.315043</td>\n",
       "      <td>75.823700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.405900</td>\n",
       "      <td>0.443490</td>\n",
       "      <td>61.229896</td>\n",
       "      <td>75.775111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.443397</td>\n",
       "      <td>61.210974</td>\n",
       "      <td>75.759836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.346200</td>\n",
       "      <td>0.443319</td>\n",
       "      <td>61.201514</td>\n",
       "      <td>75.755994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.426300</td>\n",
       "      <td>0.443271</td>\n",
       "      <td>61.210974</td>\n",
       "      <td>75.765329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>0.443246</td>\n",
       "      <td>61.210974</td>\n",
       "      <td>75.771975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training with FREEZE TUNING.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Training with FREEZE TUNING ====\\n\")\n",
    "model_name = \"t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(\"cpu\")\n",
    "\n",
    "# Freeze all parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze specific parameters - example: unfreeze the LM head only\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-freeze-tuning\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs-freeze\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model)\n",
    "\n",
    "trainer_freeze = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_freeze.train()\n",
    "model.save_pretrained(\"./t5-freeze-tuning\")\n",
    "tokenizer.save_pretrained(\"./t5-freeze-tuning\")\n",
    "print(\"Finished training with FREEZE TUNING.\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ee76f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='331' max='331' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [331/331 05:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate result with FREEZE:  {'eval_loss': 0.44324642419815063, 'eval_exact_match': 61.21097445600757, 'eval_f1': 75.77197490083633, 'eval_runtime': 324.2085, 'eval_samples_per_second': 32.602, 'eval_steps_per_second': 1.021, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "freeze_results = trainer_freeze.evaluate()\n",
    "print(\"Evaluate result with FREEZE: \", freeze_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89d62c",
   "metadata": {},
   "source": [
    "#### P-TUNING V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e78b041a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training with P-TUNING V2 ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27380' max='27380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27380/27380 1:59:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.728100</td>\n",
       "      <td>0.549748</td>\n",
       "      <td>47.672658</td>\n",
       "      <td>61.847324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>0.530380</td>\n",
       "      <td>49.697256</td>\n",
       "      <td>64.009655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.518458</td>\n",
       "      <td>50.860927</td>\n",
       "      <td>65.183149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.675700</td>\n",
       "      <td>0.509789</td>\n",
       "      <td>51.835383</td>\n",
       "      <td>66.193497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.626600</td>\n",
       "      <td>0.503337</td>\n",
       "      <td>52.289499</td>\n",
       "      <td>66.728696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.610800</td>\n",
       "      <td>0.498758</td>\n",
       "      <td>53.017975</td>\n",
       "      <td>67.414628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.567600</td>\n",
       "      <td>0.495467</td>\n",
       "      <td>53.424787</td>\n",
       "      <td>67.801053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.538700</td>\n",
       "      <td>0.493266</td>\n",
       "      <td>53.774834</td>\n",
       "      <td>68.210030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.596700</td>\n",
       "      <td>0.492019</td>\n",
       "      <td>53.945128</td>\n",
       "      <td>68.389107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.570900</td>\n",
       "      <td>0.491614</td>\n",
       "      <td>54.058657</td>\n",
       "      <td>68.530349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training with P-TUNING V2.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# P-Tuning v2\n",
    "print(\"\\n==== Training with P-TUNING V2 ====\\n\")\n",
    "model_name = \"t5-small\"\n",
    "base_model_ptv2 = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "peft_config_ptv2 = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Answer the question based on the context.\",  # Task-specific initialization\n",
    "    num_virtual_tokens=20,\n",
    "    tokenizer_name_or_path=model_name,\n",
    "    inference_mode=False,\n",
    ")\n",
    "model_ptv2 = get_peft_model(base_model_ptv2, peft_config_ptv2).to(\"cpu\")  # Force CPU\n",
    "training_args_ptv2 = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-peft-ptv2\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs-ptv2\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,  # Disable FP16 for CPU\n",
    ")\n",
    "data_collator_ptv2 = DataCollatorForSeq2Seq(tokenizer, model_ptv2, label_pad_token_id=tokenizer.pad_token_id)\n",
    "trainer_ptv2 = Seq2SeqTrainer(\n",
    "    model=model_ptv2,\n",
    "    args=training_args_ptv2,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_ptv2,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_ptv2.train()\n",
    "model_ptv2.save_pretrained(\"./t5-peft-ptv2\")\n",
    "tokenizer.save_pretrained(\"./t5-peft-ptv2\")\n",
    "print(\"Finished training with P-TUNING V2.\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "856f1b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='331' max='331' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [331/331 05:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results with P-TUNING V2: {'eval_loss': 0.4916144907474518, 'eval_exact_match': 54.058656575212865, 'eval_f1': 68.53034923827283, 'eval_runtime': 327.1992, 'eval_samples_per_second': 32.304, 'eval_steps_per_second': 1.012, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "ptv2_results = trainer_ptv2.evaluate()\n",
    "print(\"Evaluation results with P-TUNING V2:\", ptv2_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef5728",
   "metadata": {},
   "source": [
    "#### LORA FINETUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150e1308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training with LORA ====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10952' max='13690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10952/13690 1:15:57 < 18:59, 2.40 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.431055</td>\n",
       "      <td>61.286660</td>\n",
       "      <td>75.619932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.381100</td>\n",
       "      <td>0.420659</td>\n",
       "      <td>61.513718</td>\n",
       "      <td>75.968633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.418586</td>\n",
       "      <td>62.071902</td>\n",
       "      <td>76.217438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.414501</td>\n",
       "      <td>61.958373</td>\n",
       "      <td>76.028235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.368600</td>\n",
       "      <td>0.409802</td>\n",
       "      <td>62.317881</td>\n",
       "      <td>76.458878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.412830</td>\n",
       "      <td>62.563860</td>\n",
       "      <td>76.679810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.413686</td>\n",
       "      <td>62.317881</td>\n",
       "      <td>76.435071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>0.410395</td>\n",
       "      <td>62.639546</td>\n",
       "      <td>76.681935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training with LORA.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "# LoRA Training\n",
    "print(\"\\n==== Training with LORA ====\\n\")\n",
    "model_name = \"t5-small\"\n",
    "base_model_lora = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "peft_config_lora = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "model_lora = get_peft_model(base_model_lora, peft_config_lora)\n",
    "training_args_lora = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-peft-lora\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=64,\n",
    "    generation_num_beams=4,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs-lora\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "data_collator_lora = DataCollatorForSeq2Seq(tokenizer, model_lora)\n",
    "trainer_lora = Seq2SeqTrainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator_lora,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer_lora.train()\n",
    "model_lora.save_pretrained(\"./t5-peft-lora\")\n",
    "tokenizer.save_pretrained(\"./t5-peft-lora\")\n",
    "print(\"Finished training with LORA.\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5edc68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='166' max='166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [166/166 03:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate result with LORA:  {'eval_loss': 0.4098019003868103, 'eval_exact_match': 62.317880794701985, 'eval_f1': 76.4588778571315, 'eval_runtime': 260.8604, 'eval_samples_per_second': 40.52, 'eval_steps_per_second': 0.636, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "lora_results = trainer_lora.evaluate()\n",
    "print(\"Evaluate result with LORA: \", lora_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b8e35",
   "metadata": {},
   "source": [
    "### SUMMARY TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5df0c489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fine-tune Method   |   Eval Loss |   Eval Exact Match |   Eval F1 |\n",
      "|--------------------|-------------|--------------------|-----------|\n",
      "| P-Tuning v2        |      0.4916 |            54.0587 |   68.5303 |\n",
      "| Freeze             |      0.4432 |            61.211  |   75.772  |\n",
      "| LoRA               |      0.4098 |            62.3179 |   76.4589 |\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Create a table 3 fine-tune techniques\n",
    "results_table = [\n",
    "    [\"P-Tuning v2\", round(ptv2_results[\"eval_loss\"], 4), round(ptv2_results[\"eval_exact_match\"], 4), round(ptv2_results[\"eval_f1\"], 4)],\n",
    "    [\"Freeze\", round(freeze_results[\"eval_loss\"], 4), round(freeze_results[\"eval_exact_match\"], 4), round(freeze_results[\"eval_f1\"], 4)],\n",
    "    [\"LoRA\", round(lora_results[\"eval_loss\"], 4), round(lora_results[\"eval_exact_match\"], 4), round(lora_results[\"eval_f1\"], 4)]\n",
    "]\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(results_table, headers=[\"Fine-tune Method\", \"Eval Loss\" , \"Eval Exact Match\", \"Eval F1\"], tablefmt=\"github\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a364f3",
   "metadata": {},
   "source": [
    "#### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2235db92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample QA example\n",
    "# sample_question = \"What is the capital of France?\"\n",
    "# sample_context = \"France is a country in Western Europe. Its capital city is Paris, known for the Eiffel Tower.\"\n",
    "# Sample QA example (more challenging)\n",
    "sample_question = \"In what year did the Great Fire of London happen?\"\n",
    "sample_context = \"The Great Fire of London destroyed much of the city, including over 13,000 houses and St. Paul’s Cathedral. It began on Pudding Lane and lasted several days in September 1666.\"\n",
    "\n",
    "\n",
    "# Prepare input\n",
    "input_text = f\"question: {sample_question[:100]} context: {sample_context[:400]}\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Function to generate and decode answer\n",
    "def generate_answer(model, inputs, tokenizer, max_length=64, num_beams=4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67ade545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Testing LoRA Model ====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what year did the Great Fire of London happen?\n",
      "Context: The Great Fire of London destroyed much of the city, including over 13,000 houses and St. Paul’s Cathedral. It began on Pudding Lane and lasted several days in September 1666.\n",
      "LoRA Answer: 1666\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "# 1. Load and test LoRA model\n",
    "print(\"\\n==== Testing LoRA Model ====\\n\")\n",
    "peft_config_lora = PeftConfig.from_pretrained(\"./t5-peft-lora\")\n",
    "base_model_lora = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model_lora = PeftModel.from_pretrained(base_model_lora, \"./t5-peft-lora\")\n",
    "model_lora.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "answer_lora = generate_answer(model_lora, inputs, tokenizer)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"LoRA Answer: {answer_lora}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d3c4298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Testing Freeze Tuning Model ====\n",
      "\n",
      "Question: In what year did the Great Fire of London happen?\n",
      "Context: The Great Fire of London destroyed much of the city, including over 13,000 houses and St. Paul’s Cathedral. It began on Pudding Lane and lasted several days in September 1666.\n",
      "Freeze Tuning Answer: 1666\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==== Testing Freeze Tuning Model ====\\n\")\n",
    "\n",
    "# Load model như thường, không dùng PEFT\n",
    "model_freeze = AutoModelForSeq2SeqLM.from_pretrained(\"./t5-freeze-tuning\")\n",
    "model_freeze.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "answer_freeze = generate_answer(model_freeze, inputs, tokenizer)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"Freeze Tuning Answer: {answer_freeze}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d10d8a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Testing Parameter Fine-Tuning v2 Model ====\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In what year did the Great Fire of London happen?\n",
      "Context: The Great Fire of London destroyed much of the city, including over 13,000 houses and St. Paul’s Cathedral. It began on Pudding Lane and lasted several days in September 1666.\n",
      "Parameter Fine-Tuning v2 Answer: 1666\n"
     ]
    }
   ],
   "source": [
    "# 3. Load and test Parameter Fine-Tuning v2 model\n",
    "print(\"\\n==== Testing Parameter Fine-Tuning v2 Model ====\\n\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model_p_finetune_v2 = PeftModel.from_pretrained(base_model, \"./t5-peft-ptv2\")\n",
    "model_p_finetune_v2.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate answer\n",
    "answer_p_finetune_v2 = generate_answer(model_p_finetune_v2, inputs, tokenizer)\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"Parameter Fine-Tuning v2 Answer: {answer_p_finetune_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "23602b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Comparison ====\n",
      "\n",
      "Question: In what year did the Great Fire of London happen?\n",
      "Context: The Great Fire of London destroyed much of the city, including over 13,000 houses and St. Paul’s Cathedral. It began on Pudding Lane and lasted several days in September 1666.\n",
      "LoRA: 1666\n",
      "Freeze Tuning: 1666\n",
      "Parameter Fine-Tuning v2: 1666\n",
      "Expected Answer: 1666\n"
     ]
    }
   ],
   "source": [
    "# Compare answers\n",
    "print(\"\\n==== Comparison ====\\n\")\n",
    "print(f\"Question: {sample_question}\")\n",
    "print(f\"Context: {sample_context}\")\n",
    "print(f\"LoRA: {answer_lora}\")\n",
    "print(f\"Freeze Tuning: {answer_freeze}\")\n",
    "print(f\"Parameter Fine-Tuning v2: {answer_p_finetune_v2}\")\n",
    "print(f\"Expected Answer: 1666\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b4d62",
   "metadata": {},
   "source": [
    "#### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1ba40",
   "metadata": {},
   "source": [
    "Three fine-tuning strategies were evaluated on the QA task using the `t5-small` model:\n",
    "\n",
    "- **P-Tuning v2** (Eval Loss: 0.4916, Exact Match: 54.06%, F1: 68.53%)\n",
    "- **Freeze Tuning** (Eval Loss: 0.4432, Exact Match: 61.21%, F1: 75.77%)\n",
    "- **LoRA** (Eval Loss: 0.4098, Exact Match: 62.32%, F1: 76.46%)\n",
    "\n",
    "##### Summary\n",
    "\n",
    "- **LoRA** delivers the best overall performance across all metrics (lowest eval loss at 0.4098, highest exact match at 62.32%, and best F1 score at 76.46%). This shows its strong ability to adapt with minimal parameter overhead while still achieving high accuracy.\n",
    "\n",
    "- **Freeze Tuning** performs competitively (eval loss at 0.4432, exact match at 61.21%, F1 at 75.77%) even though most of the model is frozen. It is a resource-efficient option and works well when fine-tuning budgets are tight.\n",
    "\n",
    "- **P-Tuning v2**, while attractive due to its low number of trainable parameters, lags behind in performance (eval loss at 0.4916, exact match at 54.06%, F1 at 68.53%). It may require more task-specific prompt design or longer training to reach higher effectiveness.\n",
    "\n",
    "#### Inference Consistency\n",
    "\n",
    "Despite the differences in training strategy and performance metrics, all three fine-tuned models produced the **same and correct answer** on the selected QA test sample. This suggests that for some questions, once the base model is adapted even slightly, the output can converge — indicating the task may not require deep adaptation to perform accurately on simpler inputs.\n",
    "\n",
    "This consistency highlights a potential for using lighter fine-tuning methods (e.g., Freeze Tuning or P-Tuning v2) in production when latency or resource constraints are critical, especially in scenarios involving relatively straightforward queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6f1ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hifed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
