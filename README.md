# NLP_Assignment
---

## ðŸ‘¥ Group Information - Natural Language Processing Course (CO3086)

- **Group**: 05 - CC01   
- **Members**:
  - 2252898 â€“ Huynh Ngoc Van  
  - 2252434 â€“ Tran Gia Linh  
  - 2252757 â€“ Le Thi Phuong Thao  
  - 2252654 â€“ Huynh Lan Phuong  
  - 2252803 â€“ Nguyen Ngoc Song Thuong
## ðŸ“Œ Overview

This project focuses on understanding and experimenting with **Transformer-based Large Language Models (LLMs)** through the following main objectives:
- ðŸ”¹ **Discuss the evolution of LLMs**, from early models like BERT and GPT-2 to more advanced systems like GPT-4 and T5, highlighting key milestones and paradigm shifts.

- ðŸ”¹ **Understand the architecture of the Transformer**, including components such as multi-head attention, input embedding, feed-forward layers, and normalization techniques.

- ðŸ”¹ **Explore various fine-tuning techniques**, for example:
  - Full fine-tuning
  - Parameter-efficient methods such as LoRA, Prefix Tuning, P-Tuning v2
  - Lightweight approaches like Adapter tuning, Freeze tuning, and Classifier Head

- ðŸ”¹ **Implement three popular LLMs** (BERT, GPT-2, T5) and apply them to 3 real-world NLP tasks.

- ðŸ”¹ **Evaluate model performance** using metrics such as F1 score, Accuracy, Exact Match, and Loss across three NLP tasks:
  - Sentiment Analysis
  - Named Entity Recognition (NER)
  - Question Answering (QA) / Text Generation / Summarization

- ðŸ”¹ **Compare the effectiveness of different fine-tuning strategies**, focusing on both performance and parameter efficiency.

- ðŸ”¹ **Draw insights** and **Propose future directions** based on experimental results.


