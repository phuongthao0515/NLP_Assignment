{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Entity Recognition (NER)\n",
    "This notebook compares three fine-tuning techniques:\n",
    "\n",
    "- LoRA (Low-Rank Adaptation)\n",
    "- AdaLoRA (Adaptive Low-Rank Adaptation)\n",
    "- Prefix tuning\n",
    "\n",
    "Summary\n",
    "- Dataset: CoNLL-2003 dataset\n",
    "- Base Model: GPT-2\n",
    "- Evaluation Metrics: Accuracy, F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-01T10:54:31.315392Z",
     "iopub.status.busy": "2025-06-01T10:54:31.315194Z",
     "iopub.status.idle": "2025-06-01T10:55:45.670368Z",
     "shell.execute_reply": "2025-06-01T10:55:45.669656Z",
     "shell.execute_reply.started": "2025-06-01T10:54:31.315374Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate peft seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:55:45.671678Z",
     "iopub.status.busy": "2025-06-01T10:55:45.671366Z",
     "iopub.status.idle": "2025-06-01T10:56:12.139653Z",
     "shell.execute_reply": "2025-06-01T10:56:12.139098Z",
     "shell.execute_reply.started": "2025-06-01T10:55:45.671637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 10:55:57.855647: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748775358.039702      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748775358.095712      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, AdaLoraConfig, PromptTuningConfig, PrefixTuningConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:56:12.142796Z",
     "iopub.status.busy": "2025-06-01T10:56:12.141461Z",
     "iopub.status.idle": "2025-06-01T10:56:12.150671Z",
     "shell.execute_reply": "2025-06-01T10:56:12.149941Z",
     "shell.execute_reply.started": "2025-06-01T10:56:12.142768Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 1. Preprocessing ===\n",
    "def load_and_preprocess_data(num_virtual_tokens=0):\n",
    "    try:\n",
    "        # Load CoNLL-2003 dataset\n",
    "        dataset = load_dataset(\"conll2003\")\n",
    "        print(\"Dataset loaded successfully\")\n",
    "        \n",
    "        # Load tokenizer with add_prefix_space=True for pre-tokenized inputs\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"gpt2\",\n",
    "            add_prefix_space=True\n",
    "        )\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Get label list\n",
    "        label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "        num_labels = len(label_list)\n",
    "        label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "        id2label = {idx: label for idx, label in enumerate(label_list)}\n",
    "        print(f\"Labels: {label_list}\")\n",
    "        \n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(\n",
    "                examples[\"tokens\"],\n",
    "                truncation=True,\n",
    "                is_split_into_words=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "                label_ids = []\n",
    "                # Prepend -100 for virtual prompt tokens if using Prompt Tuning\n",
    "                if num_virtual_tokens > 0:\n",
    "                    label_ids.extend([-100] * num_virtual_tokens)\n",
    "                previous_word_idx = None\n",
    "                for word_idx in word_ids:\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                # Ensure label length matches input length\n",
    "                target_length = 128 + num_virtual_tokens\n",
    "                if len(label_ids) < target_length:\n",
    "                    label_ids.extend([-100] * (target_length - len(label_ids)))\n",
    "                elif len(label_ids) > target_length:\n",
    "                    label_ids = label_ids[:target_length]\n",
    "                labels.append(label_ids)\n",
    "            \n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "        \n",
    "        # Tokenize dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_and_align_labels,\n",
    "            batched=True,\n",
    "            remove_columns=dataset[\"train\"].column_names\n",
    "        )\n",
    "        print(\"Dataset tokenized successfully\")\n",
    "        \n",
    "        return tokenized_dataset, tokenizer, label_list, label2id, id2label\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in preprocessing: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pretrained Model and PEFT Configugration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:56:12.151897Z",
     "iopub.status.busy": "2025-06-01T10:56:12.151596Z",
     "iopub.status.idle": "2025-06-01T10:56:12.177317Z",
     "shell.execute_reply": "2025-06-01T10:56:12.176737Z",
     "shell.execute_reply.started": "2025-06-01T10:56:12.151852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 2. Model Setup ===\n",
    "def setup_model(num_labels, peft_type: str, label_list):\n",
    "    try:\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            \"distilgpt2\",\n",
    "            num_labels=num_labels,\n",
    "            id2label={i: label for i, label in enumerate(label_list)},\n",
    "            label2id={label: i for i, label in enumerate(label_list)}\n",
    "        )\n",
    "        \n",
    "        \n",
    "        if peft_type == \"lora\":\n",
    "            config = LoraConfig(\n",
    "                r=16,\n",
    "                lora_alpha=16,\n",
    "                target_modules=[\"c_attn\", \"c_fc\"],\n",
    "                lora_dropout=0.1,\n",
    "                bias=\"none\",\n",
    "                task_type=\"TOKEN_CLS\"\n",
    "            )\n",
    "            model = get_peft_model(model, config)\n",
    "            print(\"LoRA model configured\")\n",
    "        elif peft_type == \"adalora\":\n",
    "            config = AdaLoraConfig(\n",
    "                r=24,\n",
    "                target_r=16,\n",
    "                lora_alpha=16,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"c_attn\", \"c_fc\"],\n",
    "                task_type=\"TOKEN_CLS\",\n",
    "                inference_mode=False,\n",
    "                init_r=8,\n",
    "                tinit=200,\n",
    "                tfinal=1000,\n",
    "                deltaT=10,\n",
    "                beta1=0.85,\n",
    "                beta2=0.85,\n",
    "                modules_to_save=[\"classifier\"] \n",
    "            )\n",
    "            model = get_peft_model(model, config)\n",
    "            print(\"AdaLoRA model configured\")\n",
    "        elif peft_type == \"prefix\":\n",
    "            config = PrefixTuningConfig(\n",
    "                task_type=\"TOKEN_CLS\",\n",
    "                num_virtual_tokens=20,\n",
    "                encoder_hidden_size=768\n",
    "            )\n",
    "            model = get_peft_model(model, config)\n",
    "            print(\"Prefix Tuning model configured\")\n",
    "        elif peft_type == \"prompt_tuning\":\n",
    "            config = PromptTuningConfig(\n",
    "                task_type=\"TOKEN_CLS\",\n",
    "                num_virtual_tokens=20,\n",
    "                prompt_tuning_init=\"TEXT\",\n",
    "                prompt_tuning_init_text=\"Classify named entities in the following text:\",\n",
    "                tokenizer_name_or_path=\"distilgpt2\"\n",
    "            )\n",
    "            model = get_peft_model(model, config)\n",
    "            print(\"Prompt Tuning model configured\")\n",
    "        else:\n",
    "            print(\"Using full fine-tuning\")\n",
    "        \n",
    "        model.print_trainable_parameters()\n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model setup: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:57:57.132070Z",
     "iopub.status.busy": "2025-06-01T10:57:57.131476Z",
     "iopub.status.idle": "2025-06-01T10:57:57.139832Z",
     "shell.execute_reply": "2025-06-01T10:57:57.139025Z",
     "shell.execute_reply.started": "2025-06-01T10:57:57.132049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# === 3. Training ===\n",
    "def train_model(model, tokenized_dataset, output_dir, peft_type: str):\n",
    "    try:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=1e-4 if peft_type == \"adapter\" else 5e-5 if peft_type == \"lora\" else 1e-3,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=30,\n",
    "            weight_decay=0.1,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            report_to=\"none\",\n",
    "            gradient_accumulation_steps=2,\n",
    "            save_total_limit=1  # optional: saves space\n",
    "        )\n",
    "\n",
    "        metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "        def compute_metrics(p):\n",
    "            predictions, labels = p\n",
    "            predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "            true_labels = [\n",
    "                [label_list[l] for l in label if l != -100]\n",
    "                for label in labels\n",
    "            ]\n",
    "            pred_labels = [\n",
    "                [label_list[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "                for pred, label in zip(predictions, labels)\n",
    "            ]\n",
    "\n",
    "            results = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "            return {\n",
    "                \"precision\": results[\"overall_precision\"],\n",
    "                \"recall\": results[\"overall_recall\"],\n",
    "                \"f1\": results[\"overall_f1\"],\n",
    "                \"accuracy\": results[\"overall_accuracy\"]\n",
    "            }\n",
    "\n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStoppingCallback(\n",
    "            early_stopping_patience=3,  # stop after 3 epochs with no improvement\n",
    "            early_stopping_threshold=0.0  # requires strictly better score\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"validation\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        print(f\"Starting training for {output_dir}...\")\n",
    "        trainer.train()\n",
    "        trainer.save_model(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "        return trainer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in training: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:56:12.196959Z",
     "iopub.status.busy": "2025-06-01T10:56:12.196721Z",
     "iopub.status.idle": "2025-06-01T10:56:12.215368Z",
     "shell.execute_reply": "2025-06-01T10:56:12.214733Z",
     "shell.execute_reply.started": "2025-06-01T10:56:12.196943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 4. Evaluation and Comparison ===\n",
    "def evaluate_and_compare(trainers: Dict[str, Trainer], tokenized_dataset):\n",
    "    try:\n",
    "        print(\"Evaluating models...\")\n",
    "        results = {}\n",
    "        \n",
    "        for peft_type, trainer in trainers.items():\n",
    "            dataset = tokenized_dataset\n",
    "            eval_results = trainer.evaluate(dataset[\"test\"])\n",
    "            results[peft_type] = eval_results\n",
    "            print(f\"{peft_type.capitalize()} Model Results: {eval_results}\")\n",
    "        \n",
    "        # Compare\n",
    "        print(\"\\n=== Model Comparison ===\")\n",
    "        for peft_type, res in results.items():\n",
    "            # param_count = \"~0.1-1%\" if peft_type == \"lora\" else \"~0.5-2%\" if peft_type == \"adapter\" else \"<0.01%\" if peft_type == \"prompt_tuning\" else \"All\"\n",
    "            print(f\"{peft_type.capitalize()} - F1: {res['eval_f1']:.4f}, Accuracy: {res['eval_accuracy']:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in evaluation: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:56:12.216232Z",
     "iopub.status.busy": "2025-06-01T10:56:12.215978Z",
     "iopub.status.idle": "2025-06-01T10:56:12.235376Z",
     "shell.execute_reply": "2025-06-01T10:56:12.234694Z",
     "shell.execute_reply.started": "2025-06-01T10:56:12.216215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === 5. Demo Prediction ===\n",
    "def demo_prediction(model, tokenizer, label_list, sentence: str, num_virtual_tokens=0):\n",
    "    try:\n",
    "        model.eval()\n",
    "        # Tokenize with BatchEncoding to get word_ids\n",
    "        encoding = tokenizer(\n",
    "            sentence.split(),\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,\n",
    "            return_special_tokens_mask=True\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the same device as the model, excluding special_tokens_mask\n",
    "        inputs = {k: v.to(model.device) for k, v in encoding.items() if k != \"special_tokens_mask\"}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        predictions = torch.argmax(outputs.logits, dim=2)[0]\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "        \n",
    "        # Get word_ids from encoding\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        pred_labels = []\n",
    "        current_word_id = None\n",
    "        token_idx = num_virtual_tokens  # Skip prompt tokens\n",
    "        for word_id, pred in zip(word_ids, predictions):\n",
    "            if word_id is None or token_idx < num_virtual_tokens:\n",
    "                token_idx += 1\n",
    "                continue\n",
    "            if word_id != current_word_id:\n",
    "                pred_labels.append(label_list[pred])\n",
    "                current_word_id = word_id\n",
    "            token_idx += 1\n",
    "        \n",
    "        tokens = [token for token, wid in zip(tokens, word_ids) if wid is not None]\n",
    "        \n",
    "        print(\"=== Demo Prediction ===\")\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Token\\t\\tLabel\")\n",
    "        print(\"-----------------------\")\n",
    "        for token, label in zip(tokens, pred_labels):\n",
    "            print(f\"{token[1:]:<15}\\t{label}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in demo prediction: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:56:12.236148Z",
     "iopub.status.busy": "2025-06-01T10:56:12.235928Z",
     "iopub.status.idle": "2025-06-01T10:57:18.226076Z",
     "shell.execute_reply": "2025-06-01T10:57:18.225350Z",
     "shell.execute_reply.started": "2025-06-01T10:56:12.236132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3519b242df68465083192304d440af8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bdac067c5834e1f812d5d6e0aae5812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008df1ac285349429607ec751e85b5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ad8a2329ec461b9dc8bd7a865745c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e90c9e32b39401abc485e2845da5b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8118bc1c9163451381be215e779eba43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e719caab13549058dcb8acda12ea632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f292e9275240f58e3a32a7f47fcdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42847c97b5264b87b523c96fd8c9994d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686ed9b56cda4b3db50d4070a53dafc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99a9baf7e6d4554833bfbb5a540b574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90efbf6b2f4a9bacdf6c707e8f7bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a75c5ebc2d46c2af91ee9a41b6a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d1f01609a64ccc9cf26e4a7fde2b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenized successfully\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for LoRA and AdaLoRA\n",
    "tokenized_dataset, tokenizer, label_list, label2id, id2label = load_and_preprocess_data(\n",
    "    num_virtual_tokens=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T10:58:01.809272Z",
     "iopub.status.busy": "2025-06-01T10:58:01.808758Z",
     "iopub.status.idle": "2025-06-01T11:46:07.977951Z",
     "shell.execute_reply": "2025-06-01T11:46:07.977105Z",
     "shell.execute_reply.started": "2025-06-01T10:58:01.809250Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lora\n",
      "LoRA model configured\n",
      "trainable params: 670,473 || all params: 82,589,970 || trainable%: 0.8118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ./lora_model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24584' max='26340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24584/26340 48:04 < 03:26, 8.52 it/s, Epoch 28/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.236100</td>\n",
       "      <td>0.237861</td>\n",
       "      <td>0.656518</td>\n",
       "      <td>0.580906</td>\n",
       "      <td>0.616402</td>\n",
       "      <td>0.938116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.197100</td>\n",
       "      <td>0.190237</td>\n",
       "      <td>0.662584</td>\n",
       "      <td>0.664927</td>\n",
       "      <td>0.663753</td>\n",
       "      <td>0.949823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.165800</td>\n",
       "      <td>0.164910</td>\n",
       "      <td>0.646712</td>\n",
       "      <td>0.702138</td>\n",
       "      <td>0.673287</td>\n",
       "      <td>0.953368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151400</td>\n",
       "      <td>0.156956</td>\n",
       "      <td>0.687218</td>\n",
       "      <td>0.743223</td>\n",
       "      <td>0.714124</td>\n",
       "      <td>0.958841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>0.147393</td>\n",
       "      <td>0.702362</td>\n",
       "      <td>0.760903</td>\n",
       "      <td>0.730461</td>\n",
       "      <td>0.960770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.142526</td>\n",
       "      <td>0.714709</td>\n",
       "      <td>0.771510</td>\n",
       "      <td>0.742024</td>\n",
       "      <td>0.962698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.137800</td>\n",
       "      <td>0.137006</td>\n",
       "      <td>0.718083</td>\n",
       "      <td>0.782287</td>\n",
       "      <td>0.748811</td>\n",
       "      <td>0.963867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.136955</td>\n",
       "      <td>0.729465</td>\n",
       "      <td>0.789527</td>\n",
       "      <td>0.758308</td>\n",
       "      <td>0.965211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.130842</td>\n",
       "      <td>0.736240</td>\n",
       "      <td>0.790537</td>\n",
       "      <td>0.762423</td>\n",
       "      <td>0.965737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.130389</td>\n",
       "      <td>0.733898</td>\n",
       "      <td>0.788517</td>\n",
       "      <td>0.760227</td>\n",
       "      <td>0.965971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.123500</td>\n",
       "      <td>0.128082</td>\n",
       "      <td>0.737040</td>\n",
       "      <td>0.792389</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>0.966497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.127450</td>\n",
       "      <td>0.743497</td>\n",
       "      <td>0.798956</td>\n",
       "      <td>0.770230</td>\n",
       "      <td>0.967120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.127027</td>\n",
       "      <td>0.736924</td>\n",
       "      <td>0.799461</td>\n",
       "      <td>0.766920</td>\n",
       "      <td>0.966711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.114700</td>\n",
       "      <td>0.125849</td>\n",
       "      <td>0.737516</td>\n",
       "      <td>0.795757</td>\n",
       "      <td>0.765530</td>\n",
       "      <td>0.966613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.124111</td>\n",
       "      <td>0.744040</td>\n",
       "      <td>0.798788</td>\n",
       "      <td>0.770443</td>\n",
       "      <td>0.967178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.125231</td>\n",
       "      <td>0.738658</td>\n",
       "      <td>0.800471</td>\n",
       "      <td>0.768323</td>\n",
       "      <td>0.967315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.124401</td>\n",
       "      <td>0.742839</td>\n",
       "      <td>0.803502</td>\n",
       "      <td>0.771981</td>\n",
       "      <td>0.967841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.120734</td>\n",
       "      <td>0.744439</td>\n",
       "      <td>0.805860</td>\n",
       "      <td>0.773933</td>\n",
       "      <td>0.967977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.123044</td>\n",
       "      <td>0.742457</td>\n",
       "      <td>0.803839</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.967743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.121690</td>\n",
       "      <td>0.747303</td>\n",
       "      <td>0.804681</td>\n",
       "      <td>0.774931</td>\n",
       "      <td>0.968230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.112300</td>\n",
       "      <td>0.121911</td>\n",
       "      <td>0.749726</td>\n",
       "      <td>0.807038</td>\n",
       "      <td>0.777327</td>\n",
       "      <td>0.968230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.121340</td>\n",
       "      <td>0.748248</td>\n",
       "      <td>0.809227</td>\n",
       "      <td>0.777544</td>\n",
       "      <td>0.968211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.121127</td>\n",
       "      <td>0.746802</td>\n",
       "      <td>0.806028</td>\n",
       "      <td>0.775285</td>\n",
       "      <td>0.967938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.119905</td>\n",
       "      <td>0.747893</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>0.776185</td>\n",
       "      <td>0.968230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.120328</td>\n",
       "      <td>0.750391</td>\n",
       "      <td>0.808890</td>\n",
       "      <td>0.778543</td>\n",
       "      <td>0.968561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.106600</td>\n",
       "      <td>0.120362</td>\n",
       "      <td>0.746034</td>\n",
       "      <td>0.807712</td>\n",
       "      <td>0.775649</td>\n",
       "      <td>0.968172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.119676</td>\n",
       "      <td>0.750391</td>\n",
       "      <td>0.807880</td>\n",
       "      <td>0.778075</td>\n",
       "      <td>0.968483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.119945</td>\n",
       "      <td>0.747546</td>\n",
       "      <td>0.807712</td>\n",
       "      <td>0.776465</td>\n",
       "      <td>0.968250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./lora_model\n"
     ]
    }
   ],
   "source": [
    "# Train models with different PEFT techniques\n",
    "trainers = {}\n",
    "peft_type = \"lora\"\n",
    "print('Training with ' + peft_type)\n",
    "model = setup_model(len(label_list), peft_type=peft_type, label_list=label_list)\n",
    "trainer = train_model(model, tokenized_dataset, f\"./{peft_type}_model\", peft_type)\n",
    "trainers[peft_type] = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T11:47:01.175126Z",
     "iopub.status.busy": "2025-06-01T11:47:01.174665Z",
     "iopub.status.idle": "2025-06-01T12:36:31.650981Z",
     "shell.execute_reply": "2025-06-01T12:36:31.650254Z",
     "shell.execute_reply.started": "2025-06-01T11:47:01.175103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with adalora\n",
      "AdaLoRA model configured\n",
      "trainable params: 338,793 || all params: 82,258,302 || trainable%: 0.4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/adalora/config.py:78: UserWarning: Note that `r` is not used in AdaLora and will be ignored.If you intended to set the initial rank, use `init_r` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/tuners/adalora/model.py:204: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ./adalora_model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22828' max='26340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22828/26340 49:28 < 07:36, 7.69 it/s, Epoch 26/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.166855</td>\n",
       "      <td>0.704653</td>\n",
       "      <td>0.749621</td>\n",
       "      <td>0.726442</td>\n",
       "      <td>0.959660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.152200</td>\n",
       "      <td>0.152185</td>\n",
       "      <td>0.721264</td>\n",
       "      <td>0.760734</td>\n",
       "      <td>0.740474</td>\n",
       "      <td>0.961081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.135895</td>\n",
       "      <td>0.730372</td>\n",
       "      <td>0.786328</td>\n",
       "      <td>0.757318</td>\n",
       "      <td>0.964958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.134317</td>\n",
       "      <td>0.741966</td>\n",
       "      <td>0.793063</td>\n",
       "      <td>0.766664</td>\n",
       "      <td>0.967178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.130592</td>\n",
       "      <td>0.732135</td>\n",
       "      <td>0.795252</td>\n",
       "      <td>0.762389</td>\n",
       "      <td>0.966419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>0.121341</td>\n",
       "      <td>0.747659</td>\n",
       "      <td>0.806701</td>\n",
       "      <td>0.776059</td>\n",
       "      <td>0.968386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.116500</td>\n",
       "      <td>0.122693</td>\n",
       "      <td>0.747505</td>\n",
       "      <td>0.794578</td>\n",
       "      <td>0.770323</td>\n",
       "      <td>0.968094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.124856</td>\n",
       "      <td>0.740561</td>\n",
       "      <td>0.795925</td>\n",
       "      <td>0.767246</td>\n",
       "      <td>0.967373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.114607</td>\n",
       "      <td>0.753543</td>\n",
       "      <td>0.805691</td>\n",
       "      <td>0.778745</td>\n",
       "      <td>0.969107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.099800</td>\n",
       "      <td>0.119641</td>\n",
       "      <td>0.752454</td>\n",
       "      <td>0.813268</td>\n",
       "      <td>0.781680</td>\n",
       "      <td>0.969224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.119723</td>\n",
       "      <td>0.760635</td>\n",
       "      <td>0.806870</td>\n",
       "      <td>0.783071</td>\n",
       "      <td>0.969574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.116983</td>\n",
       "      <td>0.750981</td>\n",
       "      <td>0.805354</td>\n",
       "      <td>0.777218</td>\n",
       "      <td>0.968425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.098800</td>\n",
       "      <td>0.119921</td>\n",
       "      <td>0.757354</td>\n",
       "      <td>0.819330</td>\n",
       "      <td>0.787124</td>\n",
       "      <td>0.970470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.098300</td>\n",
       "      <td>0.117766</td>\n",
       "      <td>0.757302</td>\n",
       "      <td>0.803334</td>\n",
       "      <td>0.779639</td>\n",
       "      <td>0.969009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.093400</td>\n",
       "      <td>0.114176</td>\n",
       "      <td>0.759465</td>\n",
       "      <td>0.803839</td>\n",
       "      <td>0.781022</td>\n",
       "      <td>0.969165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.090400</td>\n",
       "      <td>0.120938</td>\n",
       "      <td>0.758722</td>\n",
       "      <td>0.823876</td>\n",
       "      <td>0.789958</td>\n",
       "      <td>0.970256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.084700</td>\n",
       "      <td>0.124933</td>\n",
       "      <td>0.760069</td>\n",
       "      <td>0.816636</td>\n",
       "      <td>0.787338</td>\n",
       "      <td>0.970314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.085700</td>\n",
       "      <td>0.113897</td>\n",
       "      <td>0.757613</td>\n",
       "      <td>0.821014</td>\n",
       "      <td>0.788040</td>\n",
       "      <td>0.970626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.115141</td>\n",
       "      <td>0.769778</td>\n",
       "      <td>0.819161</td>\n",
       "      <td>0.793703</td>\n",
       "      <td>0.971308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.086800</td>\n",
       "      <td>0.117044</td>\n",
       "      <td>0.765272</td>\n",
       "      <td>0.816299</td>\n",
       "      <td>0.789963</td>\n",
       "      <td>0.970529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.087900</td>\n",
       "      <td>0.112747</td>\n",
       "      <td>0.774152</td>\n",
       "      <td>0.818993</td>\n",
       "      <td>0.795942</td>\n",
       "      <td>0.971639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.119159</td>\n",
       "      <td>0.768640</td>\n",
       "      <td>0.826233</td>\n",
       "      <td>0.796397</td>\n",
       "      <td>0.971425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.114965</td>\n",
       "      <td>0.772577</td>\n",
       "      <td>0.825391</td>\n",
       "      <td>0.798111</td>\n",
       "      <td>0.971853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.109661</td>\n",
       "      <td>0.772828</td>\n",
       "      <td>0.823708</td>\n",
       "      <td>0.797457</td>\n",
       "      <td>0.971756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.116724</td>\n",
       "      <td>0.772248</td>\n",
       "      <td>0.825560</td>\n",
       "      <td>0.798014</td>\n",
       "      <td>0.971542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.117321</td>\n",
       "      <td>0.766386</td>\n",
       "      <td>0.826907</td>\n",
       "      <td>0.795497</td>\n",
       "      <td>0.971327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./adalora_model\n"
     ]
    }
   ],
   "source": [
    "peft_type = \"adalora\"\n",
    "print('Training with ' + peft_type)\n",
    "model = setup_model(len(label_list), peft_type=peft_type, label_list=label_list)\n",
    "trainer = train_model(model, tokenized_dataset, f\"./{peft_type}_model\", peft_type)\n",
    "trainers[peft_type] = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T12:36:31.652596Z",
     "iopub.status.busy": "2025-06-01T12:36:31.652272Z",
     "iopub.status.idle": "2025-06-01T13:21:38.721059Z",
     "shell.execute_reply": "2025-06-01T13:21:38.720474Z",
     "shell.execute_reply.started": "2025-06-01T12:36:31.652576Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with prefix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForTokenClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix Tuning model configured\n",
      "trainable params: 191,241 || all params: 82,110,738 || trainable%: 0.2329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for ./prefix_model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24584' max='26340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24584/26340 45:05 < 03:13, 9.09 it/s, Epoch 28/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.226735</td>\n",
       "      <td>0.585542</td>\n",
       "      <td>0.669641</td>\n",
       "      <td>0.624774</td>\n",
       "      <td>0.942246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199300</td>\n",
       "      <td>0.184682</td>\n",
       "      <td>0.657964</td>\n",
       "      <td>0.717124</td>\n",
       "      <td>0.686271</td>\n",
       "      <td>0.953251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.163755</td>\n",
       "      <td>0.691715</td>\n",
       "      <td>0.740865</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.958413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.150088</td>\n",
       "      <td>0.703276</td>\n",
       "      <td>0.759050</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>0.960906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.142400</td>\n",
       "      <td>0.144494</td>\n",
       "      <td>0.708053</td>\n",
       "      <td>0.762418</td>\n",
       "      <td>0.734231</td>\n",
       "      <td>0.961919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.137892</td>\n",
       "      <td>0.715987</td>\n",
       "      <td>0.769153</td>\n",
       "      <td>0.741619</td>\n",
       "      <td>0.963088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.134029</td>\n",
       "      <td>0.724791</td>\n",
       "      <td>0.773363</td>\n",
       "      <td>0.748289</td>\n",
       "      <td>0.964101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.122700</td>\n",
       "      <td>0.137211</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.775888</td>\n",
       "      <td>0.744787</td>\n",
       "      <td>0.963497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.129607</td>\n",
       "      <td>0.723434</td>\n",
       "      <td>0.776057</td>\n",
       "      <td>0.748822</td>\n",
       "      <td>0.965094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.119800</td>\n",
       "      <td>0.130118</td>\n",
       "      <td>0.727415</td>\n",
       "      <td>0.784981</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.965308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.126100</td>\n",
       "      <td>0.130612</td>\n",
       "      <td>0.732681</td>\n",
       "      <td>0.781781</td>\n",
       "      <td>0.756435</td>\n",
       "      <td>0.965250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.113300</td>\n",
       "      <td>0.129137</td>\n",
       "      <td>0.738620</td>\n",
       "      <td>0.784139</td>\n",
       "      <td>0.760699</td>\n",
       "      <td>0.965834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.122900</td>\n",
       "      <td>0.129551</td>\n",
       "      <td>0.739103</td>\n",
       "      <td>0.793736</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>0.966536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.122600</td>\n",
       "      <td>0.127986</td>\n",
       "      <td>0.744606</td>\n",
       "      <td>0.784476</td>\n",
       "      <td>0.764021</td>\n",
       "      <td>0.966380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.117500</td>\n",
       "      <td>0.125650</td>\n",
       "      <td>0.746183</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.767482</td>\n",
       "      <td>0.966594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.110200</td>\n",
       "      <td>0.131029</td>\n",
       "      <td>0.740799</td>\n",
       "      <td>0.793063</td>\n",
       "      <td>0.766040</td>\n",
       "      <td>0.966652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.104100</td>\n",
       "      <td>0.128101</td>\n",
       "      <td>0.735725</td>\n",
       "      <td>0.794073</td>\n",
       "      <td>0.763787</td>\n",
       "      <td>0.966750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.125905</td>\n",
       "      <td>0.742607</td>\n",
       "      <td>0.799124</td>\n",
       "      <td>0.769830</td>\n",
       "      <td>0.967139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.744937</td>\n",
       "      <td>0.792726</td>\n",
       "      <td>0.768089</td>\n",
       "      <td>0.967256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>0.125138</td>\n",
       "      <td>0.749920</td>\n",
       "      <td>0.792726</td>\n",
       "      <td>0.770729</td>\n",
       "      <td>0.967393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.125928</td>\n",
       "      <td>0.745191</td>\n",
       "      <td>0.795757</td>\n",
       "      <td>0.769644</td>\n",
       "      <td>0.966886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>0.124613</td>\n",
       "      <td>0.746706</td>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.773203</td>\n",
       "      <td>0.967860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.117000</td>\n",
       "      <td>0.123615</td>\n",
       "      <td>0.746574</td>\n",
       "      <td>0.798114</td>\n",
       "      <td>0.771484</td>\n",
       "      <td>0.967587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.110600</td>\n",
       "      <td>0.123382</td>\n",
       "      <td>0.747793</td>\n",
       "      <td>0.798788</td>\n",
       "      <td>0.772450</td>\n",
       "      <td>0.967685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.118100</td>\n",
       "      <td>0.123668</td>\n",
       "      <td>0.752184</td>\n",
       "      <td>0.797272</td>\n",
       "      <td>0.774072</td>\n",
       "      <td>0.968055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.744488</td>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.772012</td>\n",
       "      <td>0.967646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.123287</td>\n",
       "      <td>0.753151</td>\n",
       "      <td>0.794747</td>\n",
       "      <td>0.773390</td>\n",
       "      <td>0.967665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.745212</td>\n",
       "      <td>0.799293</td>\n",
       "      <td>0.771306</td>\n",
       "      <td>0.967626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./prefix_model\n"
     ]
    }
   ],
   "source": [
    "peft_type = \"prefix\"\n",
    "print('Training with ' + peft_type)\n",
    "model = setup_model(len(label_list), peft_type=peft_type, label_list=label_list)\n",
    "trainer = train_model(model, tokenized_dataset, f\"./{peft_type}_model\", peft_type)\n",
    "trainers[peft_type] = trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T13:21:38.722145Z",
     "iopub.status.busy": "2025-06-01T13:21:38.721843Z",
     "iopub.status.idle": "2025-06-01T13:22:07.609689Z",
     "shell.execute_reply": "2025-06-01T13:22:07.609069Z",
     "shell.execute_reply.started": "2025-06-01T13:21:38.722125Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora Model Results: {'eval_loss': 0.17044632136821747, 'eval_precision': 0.699547949628673, 'eval_recall': 0.767445979454481, 'eval_f1': 0.7319256756756757, 'eval_accuracy': 0.9578675282714055, 'eval_runtime': 9.4463, 'eval_samples_per_second': 365.54, 'eval_steps_per_second': 22.866, 'epoch': 28.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adalora Model Results: {'eval_loss': 0.18012863397598267, 'eval_precision': 0.7098344693281402, 'eval_recall': 0.7747077577045696, 'eval_f1': 0.7408536585365854, 'eval_accuracy': 0.9590522347872913, 'eval_runtime': 9.9584, 'eval_samples_per_second': 346.743, 'eval_steps_per_second': 21.69, 'epoch': 26.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix Model Results: {'eval_loss': 0.16270868480205536, 'eval_precision': 0.6991643454038997, 'eval_recall': 0.7557562876372653, 'eval_f1': 0.7263596901863989, 'eval_accuracy': 0.9576305869682283, 'eval_runtime': 9.4691, 'eval_samples_per_second': 364.659, 'eval_steps_per_second': 22.811, 'epoch': 28.0}\n",
      "\n",
      "=== Model Comparison ===\n",
      "Lora - F1: 0.7319, Accuracy: 0.9579\n",
      "Adalora - F1: 0.7409, Accuracy: 0.9591\n",
      "Prefix - F1: 0.7264, Accuracy: 0.9576\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and compare\n",
    "results = evaluate_and_compare(trainers, tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-01T13:23:37.240186Z",
     "iopub.status.busy": "2025-06-01T13:23:37.239431Z",
     "iopub.status.idle": "2025-06-01T13:23:37.296926Z",
     "shell.execute_reply": "2025-06-01T13:23:37.296187Z",
     "shell.execute_reply.started": "2025-06-01T13:23:37.240154Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LORA]\n",
      "=== Demo Prediction ===\n",
      "Sentence: Apple is planning to open a new store in London next month.\n",
      "Token\t\tLabel\n",
      "-----------------------\n",
      "Apple          \tB-ORG\n",
      "is             \tO\n",
      "planning       \tO\n",
      "to             \tO\n",
      "open           \tO\n",
      "a              \tO\n",
      "new            \tO\n",
      "store          \tO\n",
      "in             \tO\n",
      "London         \tB-LOC\n",
      "next           \tO\n",
      "month          \tO\n",
      "\n",
      "[ADALORA]\n",
      "=== Demo Prediction ===\n",
      "Sentence: Apple is planning to open a new store in London next month.\n",
      "Token\t\tLabel\n",
      "-----------------------\n",
      "Apple          \tB-ORG\n",
      "is             \tO\n",
      "planning       \tO\n",
      "to             \tO\n",
      "open           \tO\n",
      "a              \tO\n",
      "new            \tO\n",
      "store          \tO\n",
      "in             \tO\n",
      "London         \tB-LOC\n",
      "next           \tO\n",
      "month          \tO\n",
      "\n",
      "[PREFIX]\n",
      "=== Demo Prediction ===\n",
      "Sentence: Apple is planning to open a new store in London next month.\n",
      "Token\t\tLabel\n",
      "-----------------------\n",
      "Apple          \tB-ORG\n",
      "is             \tO\n",
      "planning       \tO\n",
      "to             \tO\n",
      "open           \tO\n",
      "a              \tO\n",
      "new            \tO\n",
      "store          \tO\n",
      "in             \tO\n",
      "London         \tB-LOC\n",
      "next           \tO\n",
      "month          \tO\n"
     ]
    }
   ],
   "source": [
    "# Demo prediction with LoRA model\n",
    "test_sentence = \"Apple is planning to open a new store in London next month.\"\n",
    "print('\\n[LORA]')\n",
    "demo_prediction(\n",
    "    trainers[\"lora\"].model,\n",
    "    tokenizer,\n",
    "    label_list,\n",
    "    test_sentence,\n",
    "    num_virtual_tokens=0\n",
    ")\n",
    "# Demo prediction with AdaLoRA Tuning model\n",
    "print('\\n[ADALORA]')\n",
    "demo_prediction(\n",
    "    trainers[\"adalora\"].model,\n",
    "    tokenizer,\n",
    "    label_list,\n",
    "    test_sentence,\n",
    "    num_virtual_tokens=20\n",
    ")\n",
    "# Demo prediction with Prefix Tuning model\n",
    "print('\\n[PREFIX]')\n",
    "demo_prediction(\n",
    "    trainers[\"prefix\"].model,\n",
    "    tokenizer,\n",
    "    label_list,\n",
    "    test_sentence,\n",
    "    num_virtual_tokens=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F1 Score**: **AdaLoRA** (0.7409) outperforms LoRA (0.7319) and Prefix tuning (0.7264), indicating better handling of entity recognition challenges, such as class imbalance or rare entities.\n",
    "\n",
    "- **Accuracy**: All methods achieve high accuracy (>0.957), with **AdaLoRA** slightly leading (0.9591). The small differences suggest that all models are effective for general token classification, but F1 score differences highlight varying abilities to handle entity-specific challenges.\n",
    "\n",
    "- **Efficiency**: **LoRA and AdaLoRA** are parameter-efficient fine-tuning methods, making them more computationally efficient than full fine-tuning. Prefix tuning, while also efficient, appears less effective for NER based on the F1 score."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
