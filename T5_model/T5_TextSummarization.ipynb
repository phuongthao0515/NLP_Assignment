{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf0e997",
   "metadata": {},
   "source": [
    "#### T5-SMALL - TASK: TEXT SUMMARIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe23b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3be6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f972627c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ae821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ebb0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"./checkpoint/tokenizer\"\n",
    "tokenized_dataset_path = \"./checkpoint/tokenized_dataset\"\n",
    "\n",
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c14fb",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4986196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3046b0fb2bd49c299905b6ea40cb49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd60e38af23412d97824bdbf889e44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58d253c00c4150ac947dd4ea8c1e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76eba29ccda44c4095c921b7458e6fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71898770ed4e04ba090d6baf1f0d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1368768d73f14554b0c77a3127f92e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = [prefix + doc for doc in example[\"article\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            example[\"highlights\"],\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=None\n",
    "        )\n",
    "\n",
    "    # Replace pad_token_id (typically 0) with -100 to ignore padding in loss\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "if os.path.exists(tokenized_dataset_path):\n",
    "    # print(\"Loading tokenized dataset from checkpoint...\")\n",
    "    tokenized_datasets = load_from_disk(tokenized_dataset_path)\n",
    "else:\n",
    "    tokenized_datasets = raw_datasets.map(preprocess, batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])\n",
    "    tokenized_datasets.save_to_disk(tokenized_dataset_path)\n",
    "\n",
    "# train_data = tokenized_datasets[\"train\"]\n",
    "# eval_data = tokenized_datasets[\"validation\"]\n",
    "train_data = tokenized_datasets[\"train\"]\n",
    "eval_data = tokenized_datasets[\"validation\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d79a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    "import evaluate\n",
    "\n",
    "def get_trainer(model, output_dir):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        predict_with_generate=True,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rougeL\",\n",
    "        generation_max_length=128,       # <-- set appropriate max length\n",
    "        generation_num_beams=4,  \n",
    "        report_to=\"none\",\n",
    "        local_rank=-1,  \n",
    "    )\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from torch.nn.parallel import DataParallel\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "        # Handle tuple output (common in Hugging Face models)\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        # Ensure tensors are on CPU and handle multi-GPU gathering\n",
    "        if isinstance(preds, torch.Tensor):\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                # If using DataParallel, ensure proper gathering\n",
    "                preds = preds if preds.dim() > 0 else preds.unsqueeze(0)\n",
    "            preds = preds.cpu().numpy()\n",
    "        if isinstance(labels, torch.Tensor):\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                labels = labels if labels.dim() > 0 else labels.unsqueeze(0)\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "        # Convert to lists\n",
    "        preds = preds.tolist() if isinstance(preds, np.ndarray) else preds\n",
    "        labels = labels.tolist() if isinstance(labels, np.ndarray) else labels\n",
    "\n",
    "        # Ensure token IDs are within valid range\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id  # Fallback if pad_token_id is None\n",
    "        preds = [\n",
    "            [token if token != -100 else tokenizer.pad_token_id for token in seq]\n",
    "            for seq in preds\n",
    "        ]\n",
    "        labels = [\n",
    "            [token if token != -100 else tokenizer.pad_token_id for token in seq]\n",
    "            for seq in labels\n",
    "        ]\n",
    "\n",
    "        # Clamp token IDs to valid range [0, vocab_size - 1]\n",
    "        preds = [[int(min(max(token, 0), vocab_size - 1)) for token in seq] for seq in preds]\n",
    "        labels = [[int(min(max(token, 0), vocab_size - 1)) for token in seq] for seq in labels]\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        # Compute ROUGE scores\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": round(result[\"rouge1\"], 4),\n",
    "            \"rouge2\": round(result[\"rouge2\"], 4),\n",
    "            \"rougeL\": round(result[\"rougeL\"], 4),\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf7b283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"your_command_here 2>/dev/null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb6f787",
   "metadata": {},
   "source": [
    "### Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56b8b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17948' max='89740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17948/89740 58:55 < 3:55:43, 5.08 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.669000</td>\n",
       "      <td>2.529305</td>\n",
       "      <td>0.374800</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.584600</td>\n",
       "      <td>2.432097</td>\n",
       "      <td>0.393800</td>\n",
       "      <td>0.174000</td>\n",
       "      <td>0.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.502700</td>\n",
       "      <td>2.383007</td>\n",
       "      <td>0.390300</td>\n",
       "      <td>0.172200</td>\n",
       "      <td>0.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.456800</td>\n",
       "      <td>2.346484</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.169100</td>\n",
       "      <td>0.267000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 05:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Tuning Results: {'eval_loss': 2.4320971965789795, 'eval_rouge1': 0.3938, 'eval_rouge2': 0.174, 'eval_rougeL': 0.2715, 'eval_runtime': 350.8431, 'eval_samples_per_second': 38.102, 'eval_steps_per_second': 0.596, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# 1. Prompt Tuning\n",
    "from peft import PromptTuningConfig, get_peft_model, TaskType\n",
    "\n",
    "model_pt = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model_pt.save_pretrained(\"./checkpoint_t5_small/t5-prompt-base\")\n",
    "prompt_config = PromptTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    num_virtual_tokens=20,\n",
    "    tokenizer_name_or_path=\"t5-small\"\n",
    ")\n",
    "model_pt = get_peft_model(model_pt, prompt_config)\n",
    "\n",
    "trainer_pt = get_trainer(model_pt, \"./checkpoint_t5_small/t5-prompt-tuning\")\n",
    "trainer_pt.train()\n",
    "results_pt = trainer_pt.evaluate()\n",
    "print(\"Prompt Tuning Results:\", results_pt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5cb293",
   "metadata": {},
   "source": [
    "### Layer Freezing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7a038b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22435' max='89740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22435/89740 2:49:25 < 8:28:19, 2.21 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.557900</td>\n",
       "      <td>1.543000</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.209500</td>\n",
       "      <td>0.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.534623</td>\n",
       "      <td>0.434500</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.536700</td>\n",
       "      <td>1.532105</td>\n",
       "      <td>0.435700</td>\n",
       "      <td>0.209500</td>\n",
       "      <td>0.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.520700</td>\n",
       "      <td>1.529674</td>\n",
       "      <td>0.435800</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.305400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.523400</td>\n",
       "      <td>1.529449</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.208900</td>\n",
       "      <td>0.305000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 15:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Freezing Results: {'eval_loss': 1.5321046113967896, 'eval_rouge1': 0.4357, 'eval_rouge2': 0.2095, 'eval_rougeL': 0.3054, 'eval_runtime': 961.4272, 'eval_samples_per_second': 13.904, 'eval_steps_per_second': 0.217, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# 2. Layer Freezing (freeze encoder)\n",
    "model_lf = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model_lf.save_pretrained(\"./checkpoint_t5_small/t5-layer-base\")\n",
    "for param in model_lf.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "trainer_lf = get_trainer(model_lf, \"./checkpoint_t5_small/t5-layer-freeze\")\n",
    "trainer_lf.train()\n",
    "results_lf = trainer_lf.evaluate()\n",
    "print(\"Layer Freezing Results:\", results_lf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdb78d",
   "metadata": {},
   "source": [
    "### LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5177fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17948' max='89740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17948/89740 1:07:25 < 4:29:43, 4.44 it/s, Epoch 4/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.062100</td>\n",
       "      <td>1.864986</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.193300</td>\n",
       "      <td>0.289400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.084500</td>\n",
       "      <td>1.853907</td>\n",
       "      <td>0.419000</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.051700</td>\n",
       "      <td>1.850060</td>\n",
       "      <td>0.418900</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.028100</td>\n",
       "      <td>1.846388</td>\n",
       "      <td>0.418800</td>\n",
       "      <td>0.194300</td>\n",
       "      <td>0.290700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 06:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Results: {'eval_loss': 1.8539073467254639, 'eval_rouge1': 0.419, 'eval_rouge2': 0.1944, 'eval_rougeL': 0.2908, 'eval_runtime': 402.5113, 'eval_samples_per_second': 33.211, 'eval_steps_per_second': 0.519, 'epoch': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# 3. LoRA Fine-tuning\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_lora = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model_lora.save_pretrained(\"./checkpoint_t5_small/t5-lora-base\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "\n",
    "trainer_lora = get_trainer(model_lora, \"./checkpoint_t5_small/t5-lora\")\n",
    "trainer_lora.train()\n",
    "results_lora = trainer_lora.evaluate()\n",
    "print(\"LoRA Results:\", results_lora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b6a07",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d165f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /home/thaole/miniconda3/envs/hifed/lib/python3.10/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07dad38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method            ROUGE-1    ROUGE-2    ROUGE-L\n",
      "--------------  ---------  ---------  ---------\n",
      "Prompt Tuning      0.3938     0.174      0.2715\n",
      "Layer Freezing     0.4357     0.2095     0.3054\n",
      "LoRA               0.419      0.1944     0.2908\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate([\n",
    "    [\"Prompt Tuning\", results_pt['eval_rouge1'], results_pt['eval_rouge2'], results_pt['eval_rougeL']],\n",
    "    [\"Layer Freezing\", results_lf['eval_rouge1'], results_lf['eval_rouge2'], results_lf['eval_rougeL']],\n",
    "    [\"LoRA\", results_lora['eval_rouge1'], results_lora['eval_rouge2'], results_lora['eval_rougeL']]\n",
    "], headers=[\"Method\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "194f2e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Your max_length is set to 128, but your input_length is only 67. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n",
      "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example Article: \n",
      "NASA's Perseverance rover has successfully collected samples from Mars that may contain signs of ancient microbial life. Scientists are now preparing to bring the samples back to Earth for further analysis, hoping to answer the age-old question of whether life ever existed on the red planet.\n",
      "\n",
      "\n",
      "Example Summary:\n",
      " Perseverance rover has successfully collected samples from Mars that may contain signs of ancient microbial life . Scientists are now preparing to bring the samples back to Earth for further analysis .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_lora, tokenizer=tokenizer)\n",
    "article = \"\"\"\n",
    "NASA's Perseverance rover has successfully collected samples from Mars that may contain signs of ancient microbial life. Scientists are now preparing to bring the samples back to Earth for further analysis, hoping to answer the age-old question of whether life ever existed on the red planet.\n",
    "\"\"\"\n",
    "summary = summarizer(\"summarize: \" + article, max_length=128, min_length=30, do_sample=False)\n",
    "print(\"\\nExample Article:\", article)\n",
    "print(\"\\nExample Summary:\\n\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb7228",
   "metadata": {},
   "source": [
    "#### CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de96bc",
   "metadata": {},
   "source": [
    "We compared three fine-tuning methods for the T5-small model on text summarization, evaluated by ROUGE scores:\n",
    "\n",
    "- **Layer Freezing**  \n",
    "  - ROUGE-1: **0.4357**, ROUGE-2: **0.2095**, ROUGE-L: **0.3054**  \n",
    "  - Achieved the best overall performance.  \n",
    "  - Recommended when high-quality summarization is the main goal.\n",
    "\n",
    "- **LoRA**  \n",
    "  - ROUGE-1: **0.4190**, ROUGE-2: **0.1944**, ROUGE-L: **0.2908**  \n",
    "  - Offers a strong balance between performance and efficiency.  \n",
    "  - Suitable for multitask setups or environments with limited memory.\n",
    "\n",
    "- **Prompt Tuning**  \n",
    "  - ROUGE-1: **0.3938**, ROUGE-2: **0.1740**, ROUGE-L: **0.2715**  \n",
    "  - Lowest performance among the three methods.  \n",
    "  - Extremely parameter-efficient and useful for highly resource-constrained scenarios.\n",
    "\n",
    "Choose the method based on your prioritiesâ€”quality (Layer Freezing), efficiency and scalability (LoRA), or parameter/resource constraints (Prompt Tuning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb609cb8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hifed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
